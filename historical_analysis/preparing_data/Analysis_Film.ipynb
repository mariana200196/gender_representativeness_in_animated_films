{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook aggregates the data collected via Pipeline.ipynb for each film and saves it into films.json\n",
        "An example is provided for Encanto (2021)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sqlwu6h3iSk_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from collections import Counter\n",
        "import json\n",
        "import glob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROJFF3g4zM3-"
      },
      "source": [
        "Film data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx1eaHdzzLW0"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/101dalmatians.zip -d 101dalmatians > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/aladdin.zip -d aladdin  > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/aliceinwonderland.zip -d alice > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/aristocats.zip -d aristocats > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/atlantis.zip -d atlantis  > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/bambi.zip -d bambi > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/beautyandthebeast.zip -d beauty > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/bighero.zip -d bighero > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/blackcauldron.zip -d blackcauldron > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/bolt.zip -d bolt > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/brave.zip -d brave > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/brotherbear.zip -d brotherbear > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/bugslife.zip -d bugslife > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/cars.zip -d cars > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/chickenlittle.zip -d chickenlittle > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/cinderella.zip -d cinedrella > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/coco.zip -d coco > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/dinosaur.zip -d dinosaur > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/dumbo.zip -d dumbo > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/emperor.zip -d emperor > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/encanto.zip -d encanto > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/foxhound.zip -d foxhound > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/frozen.zip -d frozen > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/hercules.zip -d hercules > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/homerange.zip -d homerange > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/hunchback.zip -d hunchback > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/ichabodtoad.zip -d ichabod > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/incredibles.zip -d incredibles > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/insideout.zip -d insideout > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/junglebook.zip -d junglebook > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/ladytramp.zip -d ladytramp > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/lilo.zip -d lilo > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/lionking.zip -d lionking > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/luca.zip -d luca > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/makeminemusic.zip -d mmm > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/mermaid.zip -d mermaid > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/moana.zip -d moana  > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/monstersinc.zip -d monstersinc > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/mousedetective.zip -d mousedetective > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/mulan.zip -d mulan > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/nemo.zip -d nemo > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/oliver.zip -d oliver > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/onward.zip -d onward > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/peterpan.zip -d peterpan > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/pinocchio.zip -d pinocchio > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/pocahontas.zip -d pocahontas > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/princessfrog.zip -d princessfrog > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/ratatouille.zip -d ratatouille > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/rayadragon.zip -d raya > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/rescuers.zip -d rescuers > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/robinhood.zip -d robinhood > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/robinsons.zip -d robinsons > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/sleepingbeauty.zip -d sleepingbeauty > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/snowwhite.zip -d snowwhite > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/soul.zip -d soul > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/swordstone.zip -d swordstone > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/tangled.zip -d tangled > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/tarzan.zip -d tarzan > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/toystory.zip -d toystory > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/treasureplanet.zip -d treasureplanet > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/up.zip -d up > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/wall-e.zip -d walle > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/winnie.zip -d winnie > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/wreckitralph.zip -d wreckitralph > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/zootopia.zip -d zootopia > /dev/null\n",
        "\n",
        "# # Sequels\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/cars-2.zip -d cars-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/cars-3.zip -d cars-3/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/frozen-2.zip -d frozen-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/incredibles-2.zip -d incredibles-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/monstersinc-2.zip -d monstersinc-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/nemo-2.zip -d nemo-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/rescuers-2.zip -d rescuers-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/toystory-2.zip -d toystory-2/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/toystory-3.zip -d toystory-3/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/toystory-4.zip -d toystory-4/out > /dev/null\n",
        "# !unzip /content/drive/MyDrive/machine_learning/Analysis/Films/wreckitralph-2.zip -d wreckitralph-2/out > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "PATH = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoAKPfiehhoS",
        "outputId": "9633621e-aae0-462a-9c5a-3a8a4d15324b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total: 1, example: .\\encanto\\out\\1-encanto_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "# Films to analyse\n",
        "film_list = []\n",
        "for f in glob.glob(f'{PATH}/*/out/*.csv'):\n",
        "    film_list.append(f)\n",
        "\n",
        "# Print total films to analyse\n",
        "print(f\"Total: {len(film_list)}, example: {film_list[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Rao05piTkY"
      },
      "source": [
        "Aggregate the data (e.g. calculate % female faces, % male faces etc. per film)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Emxti1jEl7U9"
      },
      "outputs": [],
      "source": [
        "def build_tuple(f_count, m_count):\n",
        "  l = []\n",
        "  # must be alphabetically sorted\n",
        "  for i in range(f_count):\n",
        "    l.append(\"female\")\n",
        "  for i in range(m_count):\n",
        "    l.append(\"male\")\n",
        "  return tuple(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "seb9dp-JmrzD"
      },
      "outputs": [],
      "source": [
        "# Add film info to JSON\n",
        "def write_json(new_data, filename=\"./films_test.json\"):\n",
        "  try:\n",
        "    with open(filename,'r+') as f:\n",
        "      # Load existing data into a dict.\n",
        "      file_data = json.load(f)\n",
        "      # Join new_data with file_data\n",
        "      file_data[\"films\"].append(new_data)\n",
        "      # Set file's current position at offset.\n",
        "      f.seek(0)\n",
        "      # Convert back to json.\n",
        "      json.dump(file_data, f, indent = 4)\n",
        "  except IOError:\n",
        "      # If json doesn't exist yet, create it\n",
        "      pass\n",
        "      first_data = {\"films\" : [new_data]}\n",
        "      with open(filename, \"w\") as f:\n",
        "        json.dump(first_data, f)\n",
        "\n",
        "# Flatten column\n",
        "def flatten(col):\n",
        "  arr = col.apply(lambda x: literal_eval(x)) # needed because pandas reads in list as dtype string\n",
        "  arr = arr.sum() # to flatten all items into one line\n",
        "  return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb9xN1iJif4C",
        "outputId": "e12b1ef1-d083-4357-c2b9-f85d787970d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Film dict: {'sample': 'encanto-animationscreencaps.com-4697.jpg', 'num_females': 15262, 'num_males': 9678, 'percent_females': 0.61, 'percent_males': 0.39, 'num_bbox_m_female': 4795, 'num_bbox_l_female': 10467, 'num_bbox_m_male': 3707, 'num_bbox_l_male': 5971, 'percent_bbox_m_female': 0.19, 'percent_bbox_l_female': 0.42, 'percent_bbox_m_male': 0.15, 'percent_bbox_l_male': 0.24, 'top_90percent_combinations_labels': [], 'top_90percent_combinations_count': [], 'top_90percent_combinations_percent': [], 'num_0_females': 0, 'num_1_female': 0, 'num_2_females': 0, 'num_3+_females': 0, 'percent_0_females': 0, 'percent_1_female': 0, 'percent_2_females': 0, 'percent_3+_females': 0}\n",
            "Done! Analysed 1 films.\n"
          ]
        }
      ],
      "source": [
        "for item in film_list:\n",
        "  # Prepare a dict to save the results to\n",
        "  film = {\"sample\": \"image_name\"}\n",
        "\n",
        "  # Fetch dict\n",
        "  df = pd.read_csv(f\"{item}\")\n",
        "  fname = df[\"file_name\"][0].split(\"/\")[-1]\n",
        "  film[\"sample\"] = fname\n",
        "  # New, flattened df\n",
        "  genders = flatten(df['category_id'])\n",
        "  bboxes = flatten( df['bbox'])\n",
        "  if len(genders) != len(bboxes):\n",
        "    print(f\"Not same length! Gender arr: {len(genders)}, bbox arr: {len(bboxes)}\")\n",
        "  #print(f\"Total number of faces detected: {len(genders)}\")\n",
        "  df1 = pd.DataFrame({\"bbox\": bboxes, \"gender\": genders, \"area\": 0, \"bbox_size\": \"unknown\"})\n",
        "\n",
        "  ### CALCULATE BBOX AREA ###\n",
        "  # small objects: area < 32^2\n",
        "  # medium objects: 32^2 <= area < 96^2 \n",
        "  # large objects: area >= 96^2\n",
        "  for index, bbox in enumerate(df1[\"bbox\"]):\n",
        "    h = bbox[2]\n",
        "    w = bbox[3]\n",
        "    area = int(h * w)\n",
        "    if area < 32*32:\n",
        "      df1.iloc[index, 2] = area\n",
        "      df1.iloc[index, 3] = \"small\"\n",
        "    elif area >= 32*32 and area < 96*96:\n",
        "      df1.iloc[index, 2] = area\n",
        "      df1.iloc[index, 3] = \"medium\"\n",
        "    elif area >= 96*96:\n",
        "      df1.iloc[index, 2] = area\n",
        "      df1.iloc[index, 3] = \"large\"\n",
        "    else:\n",
        "      print(f\"Problem with row: {index}\")\n",
        "\n",
        "  ### REMOVE SMALL BBOXES ###\n",
        "  df1.drop(df1[df1.bbox_size == \"small\"].index, inplace=True)\n",
        "  total = len(df1[\"gender\"])\n",
        "  #print(f\"Total number of faces without small area: {total}\")\n",
        "  # Reset \"genders\" and \"bboxes\" variables\n",
        "  genders = df1['gender']\n",
        "  bboxes = df1['bbox']\n",
        "\n",
        "  ### START ANALYSIS ###\n",
        "\n",
        "  ### FFR & MFR: count & percentage ###\n",
        "  c = Counter(genders)\n",
        "  distribution_count = dict(c)\n",
        "  distribution_percentage = {\"female\": 0, \"male\": 0}\n",
        "  for x in distribution_count.keys():\n",
        "    distribution_percentage[x] = distribution_count[x]/total\n",
        "  # Add to film dict\n",
        "  film[\"num_females\"] = distribution_count[\"female\"]\n",
        "  film[\"num_males\"] = distribution_count[\"male\"]\n",
        "  film[\"percent_females\"] = round(distribution_percentage[\"female\"], 2)\n",
        "  film[\"percent_males\"] = round(distribution_percentage[\"male\"], 2)\n",
        "  #print(f\"Film dictionary: {film}\")\n",
        "\n",
        "  ### L/ M/ S bboxes: count & percentage ###\n",
        "  counts = df1.groupby(['gender', 'bbox_size']).size().reset_index(name='count')\n",
        "  counts[\"percent\"] = counts[\"count\"] / total\n",
        "  index = counts[\"bbox_size\"].unique()\n",
        "  females = counts.loc[counts[\"gender\"]==\"female\"]\n",
        "  males = counts.loc[counts[\"gender\"]==\"male\"]\n",
        "  count_f = females[\"count\"].tolist()\n",
        "  count_m = males[\"count\"].tolist()\n",
        "  # Add to film dict\n",
        "  #film[\"num_bbox_s_female\"] = int(females.loc[females[\"bbox_size\"]==\"small\"][\"count\"])\n",
        "  film[\"num_bbox_m_female\"] = int(females.loc[females[\"bbox_size\"]==\"medium\"][\"count\"])\n",
        "  film[\"num_bbox_l_female\"] = int(females.loc[females[\"bbox_size\"]==\"large\"][\"count\"])\n",
        "  #film[\"num_bbox_s_male\"] = int(males.loc[males[\"bbox_size\"]==\"small\"][\"count\"])\n",
        "  film[\"num_bbox_m_male\"] = int(males.loc[males[\"bbox_size\"]==\"medium\"][\"count\"])\n",
        "  film[\"num_bbox_l_male\"] = int(males.loc[males[\"bbox_size\"]==\"large\"][\"count\"])\n",
        "  #film[\"percent_bbox_s_female\"] = round(float(females.loc[females[\"bbox_size\"]==\"small\"][\"percent\"]), 2)\n",
        "  film[\"percent_bbox_m_female\"] = round(float(females.loc[females[\"bbox_size\"]==\"medium\"][\"percent\"]), 2)\n",
        "  film[\"percent_bbox_l_female\"] = round(float(females.loc[females[\"bbox_size\"]==\"large\"][\"percent\"]), 2)\n",
        "  #film[\"percent_bbox_s_male\"] = round(float(males.loc[males[\"bbox_size\"]==\"small\"][\"percent\"]), 2)\n",
        "  film[\"percent_bbox_m_male\"] = round(float(males.loc[males[\"bbox_size\"]==\"medium\"][\"percent\"]), 2)\n",
        "  film[\"percent_bbox_l_male\"] = round(float(males.loc[males[\"bbox_size\"]==\"large\"][\"percent\"]), 2)\n",
        "  #print(f\"Film dictionary: {film}\")\n",
        "\n",
        "  ### Combinations ###\n",
        "  categories = df['category_id'].apply(lambda x: literal_eval(x)) # note that not flattening!\n",
        "  bboxes = df['bbox'].apply(lambda x: literal_eval(x)) # note that not flattening!\n",
        "  temp = pd.DataFrame({\"file_name\": df[\"file_name\"], \"bbox\": bboxes, \"category_id\": categories})\n",
        "  df2 = pd.DataFrame(columns=['file_name','category_id','female_count','male_count','total_count'])\n",
        "\n",
        "  # Count genders per row (i.e. frame)\n",
        "  for index, x in enumerate(df2[\"category_id\"]):\n",
        "    f = x.count(\"female\")\n",
        "    m = x.count(\"male\")\n",
        "    total = f + m  \n",
        "    df2.iloc[index, 2] = f\n",
        "    df2.iloc[index, 3] = m\n",
        "    df2.iloc[index, 4] = total\n",
        "\n",
        "  # Count frequency of gender combinations\n",
        "  combination_freq = df2.groupby(['female_count', 'male_count', 'total_count']).size().reset_index(name='frequency')\n",
        "  combination_freq.sort_values(by=['frequency'], ascending=False, inplace=True)\n",
        "\n",
        "  # Percentage calculation\n",
        "  total = combination_freq[\"frequency\"].sum()\n",
        "  combination_freq[\"percent\"] = combination_freq[\"frequency\"] / total\n",
        "\n",
        "  # Determine which combinations make up 90% of all combination types\n",
        "  threshold = 0.9\n",
        "  sum = 0\n",
        "  tuples = []\n",
        "  for index, row in combination_freq.iterrows():\n",
        "    sum = sum + row[\"percent\"]\n",
        "    if sum < threshold:\n",
        "      t = build_tuple(int(row[\"female_count\"]), int(row[\"male_count\"]))\n",
        "      tuples.append(t)\n",
        "    else:\n",
        "      break\n",
        "  #print(f\"The top {len(tuples)} combinations are responsible for {sum * 100}% of all combinations:\\n{tuples}\")\n",
        "  \n",
        "  # Top 90% of combination types\n",
        "  labels_top90 = []\n",
        "  for i, row in combination_freq[:len(tuples)].iterrows():\n",
        "    fs = int(row[\"female_count\"])\n",
        "    ms = int(row[\"male_count\"])\n",
        "    labels_top90.append(f\"{fs} female(s) & {ms} male(s)\")\n",
        "\n",
        "  count_top90 = list(combination_freq[\"frequency\"][:len(tuples)])\n",
        "  percent_top90 = list(combination_freq[\"percent\"][:len(tuples)])\n",
        "\n",
        "  ### 0 females, 1 female, 2 females, 3+ females in the top 90% of combination types ###\n",
        "  f0_count = 0\n",
        "  f1_count = 0\n",
        "  f2_count = 0\n",
        "  f3_count = 0\n",
        "\n",
        "  f0_percent = 0\n",
        "  f1_percent = 0\n",
        "  f2_percent = 0\n",
        "  f3_percent = 0\n",
        "\n",
        "  for i, row in combination_freq[:len(tuples)].iterrows():\n",
        "    fs = int(row[\"female_count\"])\n",
        "    count = row[\"frequency\"]\n",
        "    percent = row[\"percent\"]\n",
        "    if fs == 0:\n",
        "      f0_count = f0_count + count\n",
        "      f0_percent = f0_percent + percent\n",
        "    elif fs == 1:\n",
        "      f1_count = f1_count + count\n",
        "      f1_percent = f1_percent + percent\n",
        "    elif fs == 2:\n",
        "      f2_count = f2_count + count\n",
        "      f2_percent = f2_percent + percent\n",
        "    else:\n",
        "      f3_count = f3_count + count\n",
        "      f3_percent = f3_percent + percent\n",
        "\n",
        "  fs_in_combinations = [\"0 females\", \"1 female\", \"2 females\", \"3+ females\"]\n",
        "  count = [f0_count, f1_count, f2_count, f3_count]\n",
        "  percent = [f0_percent, f1_percent, f2_percent, f3_percent]\n",
        "\n",
        "  # Add to film dict\n",
        "  film[\"top_90percent_combinations_labels\"] = labels_top90\n",
        "  film[\"top_90percent_combinations_count\"] = list(map(int, count_top90))\n",
        "  film[\"top_90percent_combinations_percent\"] = [round(elem, 2) for elem in percent_top90]\n",
        "  film[\"num_0_females\"] = int(count[0])\n",
        "  film[\"num_1_female\"] = int(count[1])\n",
        "  film[\"num_2_females\"] = int(count[2])\n",
        "  film[\"num_3+_females\"] = int(count[3])\n",
        "  film[\"percent_0_females\"] = round(percent[0], 2)\n",
        "  film[\"percent_1_female\"] = round(percent[1], 2)\n",
        "  film[\"percent_2_females\"] = round(percent[2], 2)\n",
        "  film[\"percent_3+_females\"] = round(percent[3], 2)\n",
        "  print(f\"Film dict: {film}\")\n",
        "\n",
        "  # Save to file\n",
        "  write_json(film)\n",
        "\n",
        "print(f\"Done! Analysed {len(film_list)} films.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoutmYIlVYmE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tnfVNqbiiZJZ",
        "LNeQy4iE9_-d",
        "3PEANsrH95_m",
        "GV0Tjqb9F0dQ",
        "-weAHlQJOU8D",
        "cqUIzy_0varE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.0 ('detectron2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "8bda8f412847ea61373d57c688c624cb6809445208034ce0b1b245fa58ed2de4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
